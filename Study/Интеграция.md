# –ü—Å–µ–≤–¥–æ–ø–æ—Ç–æ–∫–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥

PostgreSQL —Å–∞–º –ø–æ —Å–µ–±–µ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç "–Ω–∞ –ª–µ—Ç—É" –ø–æ—Ç–æ–∫ **–∏–∑–º–µ–Ω–µ–Ω–∏–π** –ø–æ –æ–±—ã—á–Ω–æ–º—É `SELECT`.   
–û–¥–Ω–∞–∫–æ –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å **–ø—Å–µ–≤–¥–æ–ø–æ—Ç–æ–∫–æ–≤—É—é** –∏–ª–∏ **—Ä–µ–∞–ª—å–Ω–æ –ø–æ—Ç–æ–∫–æ–≤—É—é —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é** –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏.

---

## ‚úÖ –í–∞—Ä–∏–∞–Ω—Ç 1: **–ü—Å–µ–≤–¥–æ–ø–æ—Ç–æ–∫** ‚Äî "—á—Ç–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é"

> üîß –ü—Ä–æ—Å—Ç–æ, –Ω–æ –Ω–µ ¬´—Ä–µ–∞–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫¬ª. –ü–æ–¥—Ö–æ–¥–∏—Ç –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞.

–ò–¥–µ—è: —Ç—ã –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ (–∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥, –º–∏–Ω—É—Ç—É –∏ —Ç.–ø.) –≤—ã–ø–æ–ª–Ω—è–µ—à—å:

1. `SELECT * FROM table WHERE id > last_max_id`
2. –≤—Å—Ç–∞–≤–ª—è–µ—à—å —ç—Ç–æ –≤ ClickHouse
3. —Å–æ—Ö—Ä–∞–Ω—è–µ—à—å `last_max_id` (–≤ —Ñ–∞–π–ª –∏–ª–∏ –±–∞–∑—É)

üì¶ –†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ stream-–ø–æ–¥—Ö–æ–¥, –Ω–æ –±–µ–∑ Kafka –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–µ–π.

---

## ‚úÖ –í–∞—Ä–∏–∞–Ω—Ç 2: **–ü–æ—Ç–æ–∫ —á–µ—Ä–µ–∑ PostgreSQL logical replication + Debezium/Kafka**

> üöÄ –≠—Ç–æ —É–∂–µ "–Ω–∞—Å—Ç–æ—è—â–∏–π" —Å—Ç—Ä–∏–º–∏–Ω–≥.

* PostgreSQL –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **logical replication** (–∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ —Ç–∞–±–ª–∏—Ü–µ –≤ –≤–∏–¥–µ WAL)
* Debezium –º–æ–∂–µ—Ç —á–∏—Ç–∞—Ç—å —ç—Ç–æ –∏ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –≤ **Kafka**
* ClickHouse –º–æ–∂–µ—Ç –ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ Kafka-—Å—Ç—Ä–∏–º —á–µ—Ä–µ–∑ **Kafka engine**

**–°—Ö–µ–º–∞:**

```
PostgreSQL ‚Üí Debezium ‚Üí Kafka ‚Üí ClickHouse Kafka engine ‚Üí MergeTree
```

üìå –°–ª–æ–∂–Ω–æ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –±–æ–ª—å—à–∏—Ö –Ω–∞–≥—Ä—É–∑–∫–∞—Ö –∏ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ.

---

## ‚úÖ –í–∞—Ä–∏–∞–Ω—Ç 3: **PostgreSQL LISTEN/NOTIFY + Python** (—Ä–µ–∞–∫—Ç–∏–≤–Ω—ã–π polling)

> –ù–∏–∑–∫–∏–π overhead, –Ω–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω–∏–º–æ

* PostgreSQL –º–æ–∂–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Å–∏–≥–Ω–∞–ª—ã —á–µ—Ä–µ–∑ `NOTIFY`
* Python —Å `asyncpg` –∏–ª–∏ `psycopg2` –º–æ–∂–µ—Ç —Å–ª—É—à–∞—Ç—å –∏—Ö
* –ü–æ—Å–ª–µ —Å–∏–≥–Ω–∞–ª–∞ —Ç—ã –∑–∞–±–∏—Ä–∞–µ—à—å –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –ø–µ—Ä–µ–∫–ª–∞–¥—ã–≤–∞–µ—à—å

---

## üîß –®–∞–±–ª–æ–Ω "–ø—Å–µ–≤–¥–æ—Å—Ç—Ä–∏–º–µ—Ä–∞** (–≤–∞—Ä–∏–∞–Ω—Ç 1):

```python
import time
import psycopg2
import clickhouse_connect

last_id = 0
BATCH_SIZE = 1000
SLEEP_TIME = 10  # —Å–µ–∫—É–Ω–¥

pg_conn = psycopg2.connect(...)
pg_cursor = pg_conn.cursor()
ch_client = clickhouse_connect.get_client(...)

while True:
    pg_cursor.execute("SELECT id, name, value FROM source_table WHERE id > %s ORDER BY id ASC LIMIT %s", (last_id, BATCH_SIZE))
    rows = pg_cursor.fetchall()

    if rows:
        ch_client.insert('target_table', data=rows, column_names=['id', 'name', 'value'])
        last_id = rows[-1][0]
        print(f"üü¢ –í—Å—Ç–∞–≤–ª–µ–Ω–æ {len(rows)} —Å—Ç—Ä–æ–∫, last_id = {last_id}")
    else:
        print("‚è≥ –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

    time.sleep(SLEEP_TIME)
```

---

## üìå –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç `LISTEN / NOTIFY`

* –í PostgreSQL —Ç—ã –º–æ–∂–µ—à—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å:

  ```sql
  NOTIFY my_channel, 'new_data_ready';
  ```
* –ê —Å–µ—Å—Å–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–¥–ø–∏—Å–∞–Ω–∞ –Ω–∞ `LISTEN my_channel`, –ø–æ–ª—É—á–∏—Ç —ç—Ç–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ.
* –í Python —Ç—ã –º–æ–∂–µ—à—å —Å–ª—É—à–∞—Ç—å —Ç–∞–∫–∏–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ `psycopg2`.

---

## ‚úÖ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ ETL-—Å—Ü–µ–Ω–∞—Ä–∏–∏

### üî∏ 1. –í PostgreSQL: —Å–æ–∑–¥–∞—ë–º —Ç—Ä–∏–≥–≥–µ—Ä

```sql
-- –ö–∞–Ω–∞–ª, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ–º –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Å–∏–≥–Ω–∞–ª
-- (–∏–º—è –ª—é–±–æ–µ: 'data_ready', 'etl_triggered' –∏ —Ç.–ø.)

CREATE OR REPLACE FUNCTION notify_new_data()
RETURNS trigger AS $$
BEGIN
    PERFORM pg_notify('new_data', NEW.id::text);  -- –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å ID
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- –í–µ—à–∞–µ–º —Ç—Ä–∏–≥–≥–µ—Ä –Ω–∞ –Ω—É–∂–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
CREATE TRIGGER data_inserted
AFTER INSERT ON source_table
FOR EACH ROW EXECUTE FUNCTION notify_new_data();
```

---

### üî∏ 2. –í Python: —Å–ª—É—à–∞–µ–º —Å–æ–±—ã—Ç–∏—è

```python
import psycopg2
import select

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL
conn = psycopg2.connect(
    host='localhost',
    port=5432,
    database='source_db',
    user='pg_user',
    password='pg_pass'
)
conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)
cur = conn.cursor()

# –ü–æ–¥–ø–∏—Å—ã–≤–∞–µ–º—Å—è –Ω–∞ –∫–∞–Ω–∞–ª
cur.execute("LISTEN new_data;")
print("üîî –ü–æ–¥–ø–∏—Å–∞–Ω –Ω–∞ –∫–∞–Ω–∞–ª 'new_data'...")

while True:
    # –ñ–¥—ë–º —Å–æ–±—ã—Ç–∏–µ (–±–ª–æ–∫–∏—Ä—É—é—â–∏–π –≤—ã–∑–æ–≤ –Ω–∞ select.select)
    if select.select([conn], [], [], 60) == ([], [], []):
        print("‚è≥ –ù–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π (—Ç–∞–π–º–∞—É—Ç 60 —Å–µ–∫)...")
    else:
        conn.poll()
        while conn.notifies:
            notify = conn.notifies.pop(0)
            print(f"üì¨ –ü–æ–ª—É—á–µ–Ω–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ: {notify.payload}")

            # –ú–æ–∂–Ω–æ —Ç—É—Ç –≤—ã–∑—ã–≤–∞—Ç—å SELECT/INSERT ‚Üí ClickHouse
```

---

## üß† –ö—É–¥–∞ –ø—Ä–∏–º–µ–Ω–∏–º–æ

| –°—Ü–µ–Ω–∞—Ä–∏–π                                     | –ü–æ–¥—Ö–æ–¥–∏—Ç?               |
| -------------------------------------------- | ----------------------- |
| –†–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –≤—Å—Ç–∞–≤–∫—É –≤ —Ç–∞–±–ª–∏—Ü—É             | ‚úÖ –û—Ç–ª–∏—á–Ω–æ               |
| –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å near-realtime ETL                | ‚úÖ –î–∞                    |
| –ü–µ—Ä–µ–¥–∞–≤–∞—Ç—å payload (ID, –¥–∞—Ç—É –∏ —Ç.–ø.)         | ‚úÖ –ú–æ–∂–Ω–æ                 |
| –ú–∞—Å—Å–æ–≤–∞—è –º–∏–≥—Ä–∞—Ü–∏—è / —Å—Ç—Ä–∏–º–∏–Ω–≥ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤ | ‚ùå –ù–µ—Ç ‚Äî –ª—É—á—à–µ Kafka/CDC |

---

## üß© –ö–∞–∫ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≤ —Ç–≤–æ—ë–º —Å–ª—É—á–∞–µ

–¢—ã –º–æ–∂–µ—à—å —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫:

1. PostgreSQL –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç `NOTIFY` –ø–æ—Å–ª–µ –≤—Å—Ç–∞–≤–∫–∏ –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫.
2. Python-—Å–∫—Ä–∏–ø—Ç –ø–æ–ª—É—á–∞–µ—Ç —Å–∏–≥–Ω–∞–ª –∏:

   * –¥–µ–ª–∞–µ—Ç `SELECT WHERE id > last_id`
   * –ø–µ—Ä–µ–ª–∏–≤–∞–µ—Ç –≤ ClickHouse
   * –æ–±–Ω–æ–≤–ª—è–µ—Ç last\_id

---

### ‚úÖ **–ü–µ—Ä–µ–Ω–æ—Å —Å bash –Ω–∞ Python.**

Python:

* –¥–∞—ë—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –æ—à–∏–±–∫–∞–º–∏, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º, —Ç–∞–π–º-–∞—É—Ç–∞–º–∏,
* –ø—Ä–æ—â–µ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è (ETL-–ª–æ–≥–∏–∫–∞, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è),
* –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ñ–æ—Ä–º–ª–µ–Ω –∫–∞–∫ `systemd`-—Å–µ—Ä–≤–∏—Å, —Å –ª–æ–≥–∞–º–∏ –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–º,
* –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ –∏ –ª—É—á—à–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, —á–µ–º –≤—ã–∑–æ–≤—ã —á–µ—Ä–µ–∑ `bash`.

–ï—Å–ª–∏ –µ—Å—Ç—å pipeline –Ω–∞ —Ç—Ä–∏–≥–≥–µ—Ä–∞—Ö –∏ `bash`, —Ç–æ Python-–≤–µ—Ä—Å–∏—è –ø–æ–∑–≤–æ–ª–∏—Ç –≤—Å—ë —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏ –¥–µ–ª–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ (batched, idempotent, —Å –ª–æ–≥–∞–º–∏, fallback'–∞–º–∏).

---

## üîé –í–æ–ø—Ä–æ—Å—ã –ø–æ Kafka, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ

---

### **1. –ß–µ–º Kafka –ª—É—á—à–µ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–µ—Ä–µ–ª–∏–≤–∞ Python-—Å–µ—Ä–≤–∏—Å–æ–º?**

üìå **Kafka –ª—É—á—à–µ**, –µ—Å–ª–∏ —Ç–µ–±–µ –Ω—É–∂–Ω–æ:

| –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ                                  | Kafka-–ø–æ–¥—Ö–æ–¥–∏—Ç? | –ü–æ—á–µ–º—É                                     |
| ------------------------------------------- | --------------- | ------------------------------------------ |
| –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å                            | ‚úÖ –î–∞            | –¥–µ—Å—è—Ç–∫–∏ –ë–î, —Å–µ—Ä–≤–∏—Å–æ–≤, –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π         |
| –ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å, "—Ç–æ—á–Ω–æ –æ–¥–∏–Ω —Ä–∞–∑"                | ‚úÖ –î–∞            | —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ñ—Ñ—Å–µ—Ç—ã, ack, retry              |
| –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ / Reprocessing           | ‚úÖ –î–∞            | –º–æ–∂–Ω–æ —á–∏—Ç–∞—Ç—å —Å –ª—é–±–æ–≥–æ –º–µ—Å—Ç–∞                |
| –ú–Ω–æ–≥–æ–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞            | ‚úÖ –î–∞            | ClickHouse, Spark, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ‚Äî –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ |
| –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –≤ —Å–ª—É—á–∞–µ –æ—Ç–∫–∞–∑–∞ ClickHouse | ‚úÖ –î–∞            | Kafka "–ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"            |

üî∏ –¢–≤–æ–π Python-—Å–µ—Ä–≤–∏—Å ‚Äî —ç—Ç–æ **point-to-point** —Å—Ö–µ–º–∞. –ï—Å–ª–∏ ClickHouse –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –¥–∞–Ω–Ω—ã–µ —Ç–µ—Ä—è—é—Ç—Å—è, –µ—Å–ª–∏ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –æ—á–µ—Ä–µ–¥—å –∏–ª–∏ –∫—ç—à.

> Kafka –¥–∞—ë—Ç –Ω–∞–¥—ë–∂–Ω—É—é, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é –∏ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤—É—é –æ—á–µ—Ä–µ–¥—å.
> –ù–æ —Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã: Kafka-–±—Ä–æ–∫–µ—Ä—ã, Zookeeper/Redpanda, Debezium –∏–ª–∏ –ø—Ä–æ–¥—é—Å–µ—Ä—ã.

–ï—Å–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ—Å—Ç–∞—è, –∏ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞ ‚Äî –ø—Ä—è–º–æ–π –ø–æ—Ç–æ–∫ (—á–µ—Ä–µ–∑ —Ç—Ä–∏–≥–≥–µ—Ä—ã –∏–ª–∏ polling) –ø—Ä–æ—â–µ.

---

### **2. –ß—Ç–æ –±–æ–ª—å—à–µ –≥—Ä—É–∑–∏—Ç PostgreSQL: —Ç—Ä–∏–≥–≥–µ—Ä—ã –∏–ª–∏ logical replication?**

| –ú–µ—Ö–∞–Ω–∏–∑–º                             | –ù–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ PostgreSQL                                      | –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏                                                                         |
| ------------------------------------ | ----------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| üî∏ **–¢—Ä–∏–≥–≥–µ—Ä—ã**                      | –ó–∞–º–µ—Ç–Ω–∞—è –Ω–∞ **INSERT/UPDATE**, –æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ –ª–æ–≥–∏–∫–∞ —Ç—è–∂—ë–ª–∞—è | –í—ã–ø–æ–ª–Ω—è—é—Ç—Å—è **–≤–Ω—É—Ç—Ä–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏**, –∑–∞–º–µ–¥–ª—è—é—Ç `INSERT`                               |
| üî∏ **Logical replication** (WAL)     | –ß—É—Ç—å –≤—ã—à–µ idle, **–Ω–∏–∑–∫–∞—è –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ**         | WAL –≤—Å—ë —Ä–∞–≤–Ω–æ –ø–∏—à–µ—Ç—Å—è, –Ω–æ Debezium "—á–∏—Ç–∞–µ—Ç" –µ–≥–æ; –µ—Å–ª–∏ –≤—Å—ë –æ—Ç–ª–∞–∂–µ–Ω–æ ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ |
| üî∏ **SELECT polling** (–ø–æ `last_id`) | –ù–∏–∑–∫–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞, –µ—Å–ª–∏ —á–∞—Å—Ç–æ—Ç–∞ —Ä–∞–∑—É–º–Ω–∞—è                      | –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤ –¥–æ \~10 —Ç—ã—Å —Å—Ç—Ä–æ–∫/–º–∏–Ω                                          |

–ï—Å–ª–∏ –≤ —Ç—Ä–∏–≥–≥–µ—Ä–∞—Ö —Ç—ã —Ç–æ–ª—å–∫–æ **–¥–æ–±–∞–≤–ª—è–µ—à—å –∫–ª—é—á–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É** ‚Äî —ç—Ç–æ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ –¥—ë—à–µ–≤–æ. –ù–æ –≤—Å—ë —Ä–∞–≤–Ω–æ **–≤–Ω—É—Ç—Ä–∏—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–π** –≤—ã–∑–æ–≤, –∞ –∑–Ω–∞—á–∏—Ç, –º–æ–∂–µ—Ç –±—ã—Ç—å "–±—É—Ç—ã–ª–æ—á–Ω—ã–º –≥–æ—Ä–ª—ã—à–∫–æ–º" –Ω–∞ –º–∞—Å—Å–æ–≤—ã—Ö –≤—Å—Ç–∞–≤–∫–∞—Ö.

> Logical replication –¥–∞—ë—Ç **–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å** ‚Äî –∏–∑–º–µ–Ω–µ–Ω–∏—è —É–∂–µ –∑–∞–∫–æ–º–º–∏—á–µ–Ω—ã, –∏ —Ç–æ–ª—å–∫–æ –ø–æ—Ç–æ–º —á–∏—Ç–∞—é—Ç—Å—è —Ä–µ–ø–ª–∏–∫–∞—Ç–æ—Ä–æ–º.

---

### **3. –ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Kafka –±–µ–∑ logical replication –∏ Debezium, –Ω–æ —á–µ—Ä–µ–∑ —Ç—Ä–∏–≥–≥–µ—Ä—ã?**

**–î–∞, –º–æ–∂–Ω–æ!** –≠—Ç–æ **–Ω–µ —É—Ç–æ–ø–∏—è**, –∞ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å.

üìå –ü—Ä–∏–º–µ—Ä:

* PostgreSQL-—Ç—Ä–∏–≥–≥–µ—Ä –ø–∏—à–µ—Ç –∫–ª—é—á–∏ –≤ `events_table`
* Python-—Å–µ—Ä–≤–∏—Å (–∏–ª–∏ `pg_notify`-listener) —á–∏—Ç–∞–µ—Ç —ç—Ç—É —Ç–∞–±–ª–∏—Ü—É
* –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ Kafka (`kafka-python`, `confluent-kafka`)
* ClickHouse —á–∏—Ç–∞–µ—Ç –∏–∑ Kafka

–¢–∞–∫—É—é —Å—Ö–µ–º—É –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é **–±–µ–∑ Debezium**, –Ω–æ —Ç–æ–≥–¥–∞:

* —Ç—ã —Å–∞–º –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—à—å —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π
* —Å–∞–º –ø–∏—à–µ—à—å Kafka-–ø—Ä–æ–¥—é—Å–µ—Ä
* —Å–∞–º —Å–ª–µ–¥–∏—à—å –∑–∞ –ø–æ—Ä—è–¥–∫–æ–º, –¥–æ—Å—Ç–∞–≤–∫–æ–π, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π

üì¶ –¢–∞–∫–∞—è —Å—Ö–µ–º–∞ –æ—Å–æ–±–µ–Ω–Ω–æ —Ö–æ—Ä–æ—à–∞, –µ—Å–ª–∏:

* —Å–≤–æ–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ payload (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ –≤–µ—Å—å row, –∞ —Ç–æ–ª—å–∫–æ `id`)
* –Ω—É–∂–µ–Ω "—Ä—É—á–Ω–æ–π" –∫–æ–Ω—Ç—Ä–æ–ª—å –∏ –Ω–µ–±–æ–ª—å—à–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

---

## üß† –í—ã–≤–æ–¥

* ‚úÖ –ü–µ—Ä–µ–Ω–æ—Å —Å `bash` –Ω–∞ `Python` ‚Äî —Ä–∞–∑—É–º–Ω—ã–π —à–∞–≥
* ‚úÖ –ï—Å–ª–∏ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞ ‚Äî –ø—Ä—è–º–æ–π —Å–ª–∏–≤ —á–µ—Ä–µ–∑ `systemd`-—Å–µ—Ä–≤–∏—Å –Ω–∞ Python ‚Äî —Ö–æ—Ä–æ—à–∏–π –ø–æ–¥—Ö–æ–¥
* ‚úÖ –¢—Ä–∏–≥–≥–µ—Ä—ã —Ä–∞–±–æ—Ç–∞—é—Ç, –Ω–æ –º–æ–≥—É—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å; –≤ –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ `logical replication` + Kafka
* ‚úÖ Kafka –±–µ–∑ Debezium —Ä–µ–∞–ª–µ–Ω ‚Äî –º–æ–∂–Ω–æ —Å—Ç—Ä–∏–º–∏—Ç—å —Å–æ–±—ã—Ç–∏—è –∏–∑ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –Ω–∞–ø—Ä—è–º—É—é


## ‚úÖ 1. **Kafka-–ø—Ä–æ–¥—é—Å–µ—Ä –Ω–∞ Python**

(–ø—É–±–ª–∏–∫—É–µ—Ç —Å–æ–±—ã—Ç–∏—è –≤ Kafka –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL)

–û–Ω:

* —á–∏—Ç–∞–µ—Ç –Ω–æ–≤—ã–µ –∫–ª—é—á–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã —Å–æ–±—ã—Ç–∏–π (–∫—É–¥–∞ –ø–∏—à–µ—Ç —Ç—Ä–∏–≥–≥–µ—Ä),
* —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç –∏—Ö (–≤ JSON, Avro, –ø—Ä–æ—Å—Ç–æ `id`, —á—Ç–æ —É–≥–æ–¥–Ω–æ),
* –ø—É–±–ª–∏–∫—É–µ—Ç –≤ Kafka –≤ –Ω—É–∂–Ω—ã–π —Ç–æ–ø–∏–∫.

---

## ‚úÖ 2. **Kafka-–∫–æ–Ω—Å—é–º–µ—Ä –Ω–∞ Python (–∏–ª–∏ ClickHouse Kafka engine)**

(—á–∏—Ç–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –∏–∑ Kafka –∏ –≤—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ ClickHouse)

–û–Ω:

* –ø–æ–ª—É—á–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∫–ª—é—á–æ–º (–∏–ª–∏ –≤—Å–µ–π —Å—Ç—Ä–æ–∫–æ–π),
* –¥–µ–ª–∞–µ—Ç `SELECT` –∏–∑ PostgreSQL (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–æ–¥—Ç—è–Ω—É—Ç—å –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ),
* –≤—Å—Ç–∞–≤–ª—è–µ—Ç –≤ ClickHouse.

üìå –õ–∏–±–æ —Ç—ã –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –ø—Ä–æ—Å—Ç–æ –≤—Å—Ç–∞–≤–ª—è–µ—à—å `id` –≤ ClickHouse, –µ—Å–ª–∏ –æ–Ω —Å–∞–º –ø–æ—Ç–æ–º –¥–µ–ª–∞–µ—Ç JOIN, –ª–∏–±–æ –≤—Å—Ç–∞–≤–ª—è–µ—à—å –≥–æ—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.

---

### üîß –í—Å—è —Å—Ö–µ–º–∞ –ø–æ–ª—É—á–∏—Ç—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫–∞—è:

```
     [PostgreSQL]
         |
     üîÅ –¢—Ä–∏–≥–≥–µ—Ä
         ‚Üì
[events_table] (id, event_time)
         ‚Üì
 [Python Kafka Producer]
         ‚Üì
        Kafka
         ‚Üì
 [Python Kafka Consumer] ‚Üí ClickHouse
```

–ù–∏–∂–µ ‚Äî **–ø–æ–ª–Ω—ã–π –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä** –Ω–∞ –±–∞–∑–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: PostgreSQL + —Ç—Ä–∏–≥–≥–µ—Ä—ã ‚Üí Kafka ‚Üí ClickHouse, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —á–µ—Ä–µ–∑ Python.

---

## üß± **–°—Ö–µ–º–∞**:

```
PostgreSQL
  ‚îî‚îÄ‚îÄ [trigger] ‚ûù events_table (id, created_at)
        ‚îî‚îÄ‚îÄ Python Kafka Producer (—á–∏—Ç–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ Kafka)
              ‚îî‚îÄ‚îÄ Kafka
                    ‚îî‚îÄ‚îÄ Python Kafka Consumer (—á–∏—Ç–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –∏ –≤—Å—Ç–∞–≤–ª—è–µ—Ç –≤ ClickHouse)
```

---

## üì¶ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è (—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á–µ—Ä–µ–∑ `pip`):

```bash
pip install psycopg2-binary confluent-kafka clickhouse-connect
```

---

## 1Ô∏è‚É£ PostgreSQL: `events_table` –∏ —Ç—Ä–∏–≥–≥–µ—Ä

```sql
CREATE TABLE events_table (
    id INTEGER PRIMARY KEY,
    created_at TIMESTAMPTZ DEFAULT now()
);

CREATE OR REPLACE FUNCTION log_event()
RETURNS trigger AS $$
BEGIN
    INSERT INTO events_table (id) VALUES (NEW.id)
    ON CONFLICT DO NOTHING;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_capture_insert
AFTER INSERT ON source_table
FOR EACH ROW
EXECUTE FUNCTION log_event();
```

---

## 2Ô∏è‚É£ Python Kafka Producer (–∏–∑ PostgreSQL ‚Üí Kafka)

```python
# kafka_producer.py
import psycopg2
from confluent_kafka import Producer
import json
import time

# PostgreSQL config
pg_conn = psycopg2.connect(
    dbname='source_db', user='pg_user', password='pg_pass', host='localhost'
)
pg_cursor = pg_conn.cursor()

# Kafka config
producer = Producer({'bootstrap.servers': 'localhost:9092'})

def delivery_report(err, msg):
    if err is not None:
        print(f'‚ùå Delivery failed: {err}')
    else:
        print(f'‚úÖ Message delivered to {msg.topic()} [{msg.partition()}]')

while True:
    pg_cursor.execute("SELECT id FROM events_table ORDER BY id LIMIT 100")
    rows = pg_cursor.fetchall()

    if not rows:
        time.sleep(5)
        continue

    for row in rows:
        event = {"id": row[0]}
        producer.produce('pg-events', json.dumps(event).encode('utf-8'), callback=delivery_report)

    producer.flush()
    ids = [row[0] for row in rows]
    pg_cursor.execute("DELETE FROM events_table WHERE id = ANY(%s)", (ids,))
    pg_conn.commit()
```

---

## 3Ô∏è‚É£ Python Kafka Consumer ‚Üí ClickHouse –≤—Å—Ç–∞–≤–∫–∞

```python
# kafka_consumer.py
from confluent_kafka import Consumer
import json
import psycopg2
import clickhouse_connect

# PostgreSQL –¥–ª—è –≤—ã—Ç–∞—Å–∫–∏–≤–∞–Ω–∏—è –ø–æ–ª–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
pg_conn = psycopg2.connect(
    dbname='source_db', user='pg_user', password='pg_pass', host='localhost'
)
pg_cursor = pg_conn.cursor()

# ClickHouse
ch_client = clickhouse_connect.get_client(host='localhost')

# Kafka consumer config
consumer = Consumer({
    'bootstrap.servers': 'localhost:9092',
    'group.id': 'pg2click-consumer',
    'auto.offset.reset': 'earliest'
})
consumer.subscribe(['pg-events'])

while True:
    msg = consumer.poll(1.0)
    if msg is None:
        continue
    if msg.error():
        print("‚ùå Consumer error:", msg.error())
        continue

    try:
        payload = json.loads(msg.value().decode('utf-8'))
        record_id = payload['id']

        pg_cursor.execute("SELECT id, name, value FROM source_table WHERE id = %s", (record_id,))
        row = pg_cursor.fetchone()
        if row:
            ch_client.insert('target_table', [row], column_names=['id', 'name', 'value'])
            print(f"‚úÖ Inserted {row} into ClickHouse")

    except Exception as e:
        print(f"‚ö†Ô∏è Error: {e}")

consumer.close()
```

---

## 4Ô∏è‚É£ ClickHouse: —Ü–µ–ª–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞

```sql
CREATE TABLE target_table (
    id UInt32,
    name String,
    value Float64
) ENGINE = MergeTree() ORDER BY id;
```

---

## ‚úÖ –ß—Ç–æ —Ç—ã –ø–æ–ª—É—á–∞–µ—à—å

* üîÅ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π —á–µ—Ä–µ–∑ `trigger`
* üîÑ –ù–∞–¥—ë–∂–Ω–∞—è –æ—á–µ—Ä–µ–¥—å –¥–æ—Å—Ç–∞–≤–∫–∏ —á–µ—Ä–µ–∑ Kafka
* üì• –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ –≤ ClickHouse
* üß© –ì–∏–±–∫–æ—Å—Ç—å: –º–æ–∂–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –∏–ª–∏ –ø–æ–¥–∫–ª—é—á–∞—Ç—å –¥—Ä—É–≥–∏–µ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–∏ (–ª–æ–≥, –∞–ª–µ—Ä—Ç—ã, BI –∏ —Ç.–¥.)

---

**–î–ª—è —Å–±–æ—Ä–∫–∏ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞ –æ–ø–∏—à–µ–º –µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –∫–æ–¥ –≤ –≤–∏–¥–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤.**

---

## üì¶ **–û–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ "PostgreSQL ‚Üí Kafka ‚Üí ClickHouse"**

### üîπ –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:

* ‚úÖ `systemd` —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (—Å–ª—É–∂–±—ã producer/consumer)
* ‚úÖ `.env` –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–ª—è –≤—Å–µ—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
* ‚úÖ Kafka offset —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (—á–µ—Ä–µ–∑ Kafka group)
* ‚úÖ Batch insert –≤ ClickHouse
* ‚úÖ –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ —á—Ç–µ–Ω–∏–µ Kafka –ø—Ä–∏ —Å–±–æ–µ
* ‚úÖ –ù–∞–¥—ë–∂–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
* ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω

---

## üß© 1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
pg_to_click/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ kafka_producer.py
‚îú‚îÄ‚îÄ kafka_consumer.py
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ systemd/
‚îÇ   ‚îú‚îÄ‚îÄ producer.service
‚îÇ   ‚îî‚îÄ‚îÄ consumer.service
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üîß –ü—Ä–∏–º–µ—Ä `.env`

```ini
# PostgreSQL
PG_HOST=localhost
PG_DB=source_db
PG_USER=pg_user
PG_PASSWORD=pg_pass

# Kafka
KAFKA_BOOTSTRAP=localhost:9092
KAFKA_TOPIC=pg-events
KAFKA_GROUP=pg2click-consumer

# ClickHouse
CH_HOST=localhost
CH_TABLE=target_table
```

---

## üìò `config.py` (–∑–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è)

```python
from dotenv import load_dotenv
import os

load_dotenv()

PG_CONFIG = {
    'host': os.getenv('PG_HOST'),
    'dbname': os.getenv('PG_DB'),
    'user': os.getenv('PG_USER'),
    'password': os.getenv('PG_PASSWORD')
}

KAFKA_CONFIG = {
    'bootstrap.servers': os.getenv('KAFKA_BOOTSTRAP'),
    'topic': os.getenv('KAFKA_TOPIC'),
    'group.id': os.getenv('KAFKA_GROUP'),
    'auto.offset.reset': 'earliest'
}

CH_CONFIG = {
    'host': os.getenv('CH_HOST'),
    'table': os.getenv('CH_TABLE')
}
```

---

## ‚öôÔ∏è `systemd` —Å–µ—Ä–≤–∏—Å—ã

### producer.service

```ini
[Unit]
Description=Postgres-to-Kafka Producer
After=network.target

[Service]
User=etluser
WorkingDirectory=/opt/pg_to_click/
ExecStart=/opt/pg_to_click/venv/bin/python kafka_producer.py
Restart=always
Environment="PYTHONUNBUFFERED=1"

[Install]
WantedBy=multi-user.target
```

### consumer.service

```ini
[Unit]
Description=Kafka-to-ClickHouse Consumer
After=network.target

[Service]
User=etluser
WorkingDirectory=/opt/pg_to_click/
ExecStart=/opt/pg_to_click/venv/bin/python kafka_consumer.py
Restart=always
Environment="PYTHONUNBUFFERED=1"

[Install]
WantedBy=multi-user.target
```

---

## üíæ Kafka producer (batch, —Å –ª–æ–≥–∞–º–∏, .env)

```python
# kafka_producer.py
import psycopg2
from confluent_kafka import Producer
from config import PG_CONFIG, KAFKA_CONFIG
import json, time, logging

logging.basicConfig(level=logging.INFO)
pg_conn = psycopg2.connect(**PG_CONFIG)
pg_cursor = pg_conn.cursor()

producer = Producer({'bootstrap.servers': KAFKA_CONFIG['bootstrap.servers']})

def delivery_report(err, msg):
    if err:
        logging.error(f'‚ùå Delivery failed: {err}')
    else:
        logging.info(f'‚úÖ Delivered: {msg.value().decode()}')

while True:
    pg_cursor.execute("SELECT id FROM events_table ORDER BY id LIMIT 100")
    rows = pg_cursor.fetchall()

    if not rows:
        time.sleep(5)
        continue

    for row in rows:
        event = {'id': row[0]}
        producer.produce(KAFKA_CONFIG['topic'], json.dumps(event).encode(), callback=delivery_report)

    producer.flush()
    pg_cursor.execute("DELETE FROM events_table WHERE id = ANY(%s)", ([r[0] for r in rows],))
    pg_conn.commit()
```

---

## üì• Kafka consumer (batch insert –≤ ClickHouse)

```python
# kafka_consumer.py
from confluent_kafka import Consumer
from config import PG_CONFIG, KAFKA_CONFIG, CH_CONFIG
import psycopg2, clickhouse_connect, json, logging

logging.basicConfig(level=logging.INFO)
pg_conn = psycopg2.connect(**PG_CONFIG)
pg_cursor = pg_conn.cursor()
ch_client = clickhouse_connect.get_client(host=CH_CONFIG['host'])

consumer = Consumer({
    'bootstrap.servers': KAFKA_CONFIG['bootstrap.servers'],
    'group.id': KAFKA_CONFIG['group.id'],
    'auto.offset.reset': KAFKA_CONFIG['auto.offset.reset']
})
consumer.subscribe([KAFKA_CONFIG['topic']])

batch = []
while True:
    msg = consumer.poll(1.0)
    if msg is None:
        continue
    if msg.error():
        logging.warning(f"Consumer error: {msg.error()}")
        continue

    payload = json.loads(msg.value().decode())
    pg_cursor.execute("SELECT id, name, value FROM source_table WHERE id = %s", (payload['id'],))
    row = pg_cursor.fetchone()
    if row:
        batch.append(row)

    if len(batch) >= 100:
        ch_client.insert(CH_CONFIG['table'], batch, column_names=['id', 'name', 'value'])
        logging.info(f"Inserted batch of {len(batch)} rows")
        batch.clear()
```

---

## üß™ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ replay

Kafka —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ñ—Ñ—Å–µ—Ç –ø–æ `group.id`. –ï—Å–ª–∏ —Ç–µ–±–µ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å "replay":

* —Å–º–µ–Ω–∏ `group.id` (–Ω–∞–ø—Ä–∏–º–µ—Ä, `pg2click-replay`)
* –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π `consumer.seek()` –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ñ—Ñ—Å–µ—Ç–æ–º

# GitHub-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å –ø–æ–ª–Ω—ã–º –ø—Ä–∏–º–µ—Ä–æ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ PostgreSQL ‚Üí Kafka ‚Üí ClickHouse –Ω–∞ Python
> –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–∞–Ω–Ω—ã—Ö.

---

## üõ†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
pg_to_click/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ kafka_producer.py
‚îú‚îÄ‚îÄ kafka_consumer.py
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ systemd/
‚îÇ   ‚îú‚îÄ‚îÄ producer.service
‚îÇ   ‚îî‚îÄ‚îÄ consumer.service
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üîß –®–∞–≥–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ GitHub

1. **–£—Å—Ç–∞–Ω–æ–≤–∏ GitHub CLI**:

   –î–ª—è Windows:

   ```bash
   winget install --id GitHub.cli
   ```

   –î–ª—è macOS:

   ```bash
   brew install gh
   ```

2. **–í—ã–ø–æ–ª–Ω–∏ –≤—Ö–æ–¥ –≤ GitHub**:

   ```bash
   gh auth login
   ```

   –°–ª–µ–¥—É–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –Ω–∞ —ç–∫—Ä–∞–Ω–µ –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏.

3. **–°–æ–∑–¥–∞–π –Ω–æ–≤—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏ –∫–ª–æ–Ω–∏—Ä—É–π –µ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ**:

   ```bash
   mkdir pg_to_click
   cd pg_to_click
   gh repo create pg_to_click --public --clone
   ```

4. **–°–æ–∑–¥–∞–π –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ –∞–∫—Ç–∏–≤–∏—Ä—É–π –µ–≥–æ**:

   ```bash
   python -m venv venv
   .\venv\Scripts\activate  # –î–ª—è Windows
   source venv/bin/activate  # –î–ª—è macOS/Linux
   ```

5. **–£—Å—Ç–∞–Ω–æ–≤–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**:

   ```bash
   pip install psycopg2-binary confluent-kafka clickhouse-connect python-dotenv
   ```

6. **–°–æ–∑–¥–∞–π —Ñ–∞–π–ª `requirements.txt`**:

   ```bash
   pip freeze > requirements.txt
   ```

7. **–î–æ–±–∞–≤—å —Ñ–∞–π–ª—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏ —Å–¥–µ–ª–∞–π –ø–µ—Ä–≤—ã–π –∫–æ–º–º–∏—Ç**:

   ```bash
   git add .
   git commit -m "Initial commit"
   ```

8. **–û—Ç–ø—Ä–∞–≤—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞ GitHub**:

   ```bash
   git push -u origin main
   ```

---

## üìÅ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤

* **`config.py`**: –ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ `.env` –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏—Ö –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–æ–¥–µ.

* **`kafka_producer.py`**: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ PostgreSQL –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Ö –≤ Kafka.

* **`kafka_consumer.py`**: –ü–æ—Ç—Ä–µ–±–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Kafka –∏ –≤—Å—Ç–∞–≤–ª—è–µ—Ç –∏—Ö –≤ ClickHouse.

* **`systemd/producer.service` –∏ `systemd/consumer.service`**: –°–∏—Å—Ç–µ–º–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ producer –∏ consumer.

---

# –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä —Ñ–∞–π–ª–æ–≤ —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –¥–ª—è —Ç–≤–æ–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞. 
> –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Ö –ª–æ–∫–∞–ª—å–Ω–æ, –∑–∞—Ç–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å git –∏ –∑–∞–ª–∏—Ç—å –Ω–∞ GitHub.

---

# 1. –§–∞–π–ª `.env`

```ini
PG_HOST=localhost
PG_DB=source_db
PG_USER=pg_user
PG_PASSWORD=pg_pass

KAFKA_BOOTSTRAP=localhost:9092
KAFKA_TOPIC=pg-events
KAFKA_GROUP=pg2click-consumer

CH_HOST=localhost
CH_TABLE=target_table
```

---

# 2. –§–∞–π–ª `config.py`

```python
from dotenv import load_dotenv
import os

load_dotenv()

PG_CONFIG = {
    'host': os.getenv('PG_HOST'),
    'dbname': os.getenv('PG_DB'),
    'user': os.getenv('PG_USER'),
    'password': os.getenv('PG_PASSWORD')
}

KAFKA_CONFIG = {
    'bootstrap.servers': os.getenv('KAFKA_BOOTSTRAP'),
    'topic': os.getenv('KAFKA_TOPIC'),
    'group.id': os.getenv('KAFKA_GROUP'),
    'auto.offset.reset': 'earliest'
}

CH_CONFIG = {
    'host': os.getenv('CH_HOST'),
    'table': os.getenv('CH_TABLE')
}
```

---

# 3. –§–∞–π–ª `kafka_producer.py`

```python
import psycopg2
from confluent_kafka import Producer
from config import PG_CONFIG, KAFKA_CONFIG
import json
import time
import logging

logging.basicConfig(level=logging.INFO)
pg_conn = psycopg2.connect(**PG_CONFIG)
pg_cursor = pg_conn.cursor()

producer = Producer({'bootstrap.servers': KAFKA_CONFIG['bootstrap.servers']})

def delivery_report(err, msg):
    if err:
        logging.error(f'‚ùå Delivery failed: {err}')
    else:
        logging.info(f'‚úÖ Delivered: {msg.value().decode()}')

while True:
    pg_cursor.execute("SELECT id FROM events_table ORDER BY id LIMIT 100")
    rows = pg_cursor.fetchall()

    if not rows:
        time.sleep(5)
        continue

    for row in rows:
        event = {'id': row[0]}
        producer.produce(KAFKA_CONFIG['topic'], json.dumps(event).encode(), callback=delivery_report)

    producer.flush()
    pg_cursor.execute("DELETE FROM events_table WHERE id = ANY(%s)", ([r[0] for r in rows],))
    pg_conn.commit()
```

---

# 4. –§–∞–π–ª `kafka_consumer.py`

```python
from confluent_kafka import Consumer
from config import PG_CONFIG, KAFKA_CONFIG, CH_CONFIG
import psycopg2
import clickhouse_connect
import json
import logging

logging.basicConfig(level=logging.INFO)
pg_conn = psycopg2.connect(**PG_CONFIG)
pg_cursor = pg_conn.cursor()
ch_client = clickhouse_connect.get_client(host=CH_CONFIG['host'])

consumer = Consumer({
    'bootstrap.servers': KAFKA_CONFIG['bootstrap.servers'],
    'group.id': KAFKA_CONFIG['group.id'],
    'auto.offset.reset': KAFKA_CONFIG['auto.offset.reset']
})
consumer.subscribe([KAFKA_CONFIG['topic']])

batch = []
while True:
    msg = consumer.poll(1.0)
    if msg is None:
        continue
    if msg.error():
        logging.warning(f"Consumer error: {msg.error()}")
        continue

    payload = json.loads(msg.value().decode())
    pg_cursor.execute("SELECT id, name, value FROM source_table WHERE id = %s", (payload['id'],))
    row = pg_cursor.fetchone()
    if row:
        batch.append(row)

    if len(batch) >= 100:
        ch_client.insert(CH_CONFIG['table'], batch, column_names=['id', 'name', 'value'])
        logging.info(f"Inserted batch of {len(batch)} rows")
        batch.clear()
```

---

# 5. –§–∞–π–ª `requirements.txt`

```
psycopg2-binary
confluent-kafka
clickhouse-connect
python-dotenv
```

---

# 6. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ‚Äî systemd —Å–µ—Ä–≤–∏—Å—ã

### producer.service

```ini
[Unit]
Description=Postgres-to-Kafka Producer
After=network.target

[Service]
User=etluser
WorkingDirectory=/path/to/pg_to_click
ExecStart=/path/to/pg_to_click/venv/bin/python kafka_producer.py
Restart=always
Environment="PYTHONUNBUFFERED=1"

[Install]
WantedBy=multi-user.target
```

### consumer.service

```ini
[Unit]
Description=Kafka-to-ClickHouse Consumer
After=network.target

[Service]
User=etluser
WorkingDirectory=/path/to/pg_to_click
ExecStart=/path/to/pg_to_click/venv/bin/python kafka_consumer.py
Restart=always
Environment="PYTHONUNBUFFERED=1"

[Install]
WantedBy=multi-user.target
```

---

# –ö–∞–∫ –±—ã—Å—Ç—Ä–æ –∑–∞–ª–∏—Ç—å –Ω–∞ GitHub:

```bash
git init
git add .
git commit -m "Initial commit with pg->kafka->clickhouse pipeline"
gh repo create my-pg-to-click --public --source=. --remote=origin
git push -u origin main
```
